%
\chapter{Fundamentals}\label{cha:Fundamentals}
%
In this chapter, the fundamentals of lane detection will be explained. All methods, which are used in this thesis, will be theoretically focused and the use of their functions in software will be also explained. Also, some advantages and disadvantages of methods will be discussed.
%
\section{Properties of Track at Carolo-Cup}\label{sec:Properties of Truck at Carolo-Cup}
%
The Carolo-Cup is an annual competition at the Technical University of Braunschweig which are attended by students. Every year the track and some properties of the competition are changing. For example, in the competitions until 2017 there was no traffic sign, but starting in 2017 there are also some traffic signs, speed limit zones, blocked areas and crosswalks for pedestrian. Because of this, in the competitions until 2017, there was only one way to understand who had the right of way. If there is a stop line on the road in front of an intersection, it means the car has to wait until the intersection is free. In the competitions starting from 2017, there are different types of intersections: They are 'Intersections with stop lines', 'Intersections with give-way lines', 'Intersections with priority to right', 'Enforced crossing direction - give-way condition', 'Enforced crossing direction - right of way condition'. Except 'Intersections with priority to right', they all have traffic signs stating who has priority. If there is a no traffic sign, it means the right side always has priority.\cite{Carolo_Cup}



\begin{figure}[H]
 \centering
  \includegraphics[width=0.75\textwidth]{./Bilder/Intersections.png}
  \caption{Left: Markings for sharp turns at Carolo-Cup.
Right: Intersections with stop lines at Carolo-Cup\cite{Carolo_Cup}}
  \label{fig:Intersections}
\end{figure}



%
\section{Inverse Perspective Mapping}\label{sec:Inverse Perspective Mapping}
%
Inverse Perspective Mapping(IPM) is an algorithm which is able to obtain accurate bird's-eye view images from forward looking cameras. With the IPM algorithm, each image pixel is remapped, and a new array of pixels is created where the lines in perspective are transformed into straight lines and objects are distorted. IPM is one of the most used methods in lane detection. In lane detection, IPM ensures that the lanes are shown vertical and parallel to each other. On the other hand, because of the re-mapping of pixels, IPM is a computationally expensive method. Because of this reason, in some cases in this master's thesis, rather than remapping all pixels of the images, only the pixels relevant to the lane and accordingly, the fitted curve, were remapped. Thanks to this, in some cases, a lot of computing time was saved.

In order to use the IPM method, the intrinsic and extrinsic parameters of camera are necessary to process images for coordinate transformation and calibration.

\begin{itemize}

\item \textbf{Intrinsic Parameters :} Intrinsic parameters are camera-specific. It includes
information of the focal length ($f_x$, $f_y$) and optical centers ($c_x$, $c_y$). It is also called a camera matrix. Although the intrinsic parameters are camera-specific, once the camera is calibrated, the modified intrinsic parameters can be stored for future purposes. It is expressed as a 3x3 matrix:

 \begin{center}
  camera matrix =  $
 \begin{bmatrix} 
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1 \\
\end{bmatrix}
$  \end{center}


In order to find the parameters of the camera matrix, the camera must be calibrated. In order to do this, there is a node in \textbf{R}obotic \textbf{O}peratic \textbf{S}ystem(ROS) which is programmed in the Python programming language. In the tutorial for camera calibration\ref{Camera_Calibration} an 8x6 checkerboard with 108mm squares is used. The following command must be used for the calibrating the camera. 

\textit{ rosrun camera$ \_ $calibration cameracalibrator.py $ \-- $size 8x6 $ \-- $square 0.108 image:=/usb$ \_ $cam/image$ \_ $raw camera:=/usb$ \_ $cam}

As seen in the above command, the number and the size of the checkerboard's squares must be written in the command and then the camera starts automatically after the command is run. This opens up the calibration window which then highlights the checkerboard. The checkerboard must be moved around in front of the camera. During this process, the camera takes some measurements from the checkerboard. When enough data is collected, the \textit{CALIBRATE} button is highlighted. After this button is clicked, the camera matrix is shown and simultaneously saved as the instrinsic parameters in the camera.


\begin{figure}[H]
 \centering
  \includegraphics[width=0.8\textwidth]{./Bilder/Camera_Calibration_fig.png}
  \caption{Calibration of Camera\cite{Camera_Calibration}}
  \label{Calibration_of_Camera}
\end{figure}



For this master's thesis, the camera was calibrated and the parameters of the camera matrix can be found in the Table \ref{tab:parameters}


\item \textbf{Extrinsic Parameters :} Extrinsic parameters are dependent on the camera position. The parameters are H and $\theta$. H is the distance between the camera and ground. $\theta$ is the camera tilt angle. These values must be measured because they have to be used for Inverse Perspective Mapping. The extrinsic parameters of the camera can be found in the Table \ref{tab:parameters}. 
 
\end{itemize}
  
  


\begin{figure}[H]
\centering
  \includegraphics[width=0.8\textwidth]{./Bilder/Related_positions_of_the_camera.png}
  \caption{Related Positions of the Camera \cite{IPM}}
  \label{fig:Procedures_of_IPM1}
\end{figure}

 As seen at Figure \ref{fig:Procedures_of_IPM1}, the camera on the car has field of view 2 (FOV2) at the real position of the camera but in this case, the view is not a bird's-eye view, so if the same field of view is to be observed from a bird's-eye view, IPM will virtually change the position to Virtual Position 2 of the camera. In this case, although the camera is at its real position, it will appear as though it is at Virtual Position 2. Sometimes, however, obtaining the view from Position 2 is not desirable, because if the field of view is very large, then Position 2 is also very high. In this project, if the lanes observed from a position high above the ground, the lanes will appear small, and thus lane detection becomes more difficult and less accurate. To solve this problem, the upper portion of the image can be cropped, so the camera will have a smaller field of view. In this case, although the camera is at its real position, it will appear as though it is at Virtual Position 1.
  
In order to apply IPM algorithm, the image coordinates must also be changed. Below, the steps of IPM calculations from the paper of \cite{IPM} will be detailed.


In order to calculate the destination image coordinates (x*,y*), the input image coordinates (x,y), the height of the camera from the ground (H), the focal length of the camera (f), and the tilt angle of the camera ($\theta$) are all necessary and are used in the following formula\cite{IPM}:
 
\begin{center}
\scalebox{1.15}{ $x^* = H \frac{x sin \theta + f cos \theta}{-y cos \theta + f sin \theta}$ ;
 $y^* = H \frac{y sin \theta + f cos \theta}{-y cos \theta + f sin \theta}$}
\end{center}

In this equation, the transformed component values of $x^*$ and $y^*$ may be less than or equal to zero. Because of 
this reason, a constant d is defined as\cite{IPM} :
 \begin{center}
\scalebox{1.15}{d = 
 $\begin{vmatrix}
 \frac{H(sin \theta + f cos \theta)}{f sin \theta - cos \theta}
 \end{vmatrix}$ + 1}
\end{center}

This means that the coordinate point in the original source image has been mapped into the point of the destination image coordinate system. Below there is the proposed equation\cite{IPM} :
 
 \begin{center}
\scalebox{1.15}{ $x^* = H \frac{x sin \theta + f cos \theta}{-y cos \theta + f sin \theta}$ + d ,
 $y^* = H \frac{y sin \theta + f cos \theta}{-y cos \theta + f sin \theta}$ + d ,
 where d = 
 $\begin{vmatrix}
 \frac{H(sin \theta + f cos \theta)}{f sin \theta - cos \theta}
 \end{vmatrix}$ + 1}
\end{center}

\begin{figure}[H]
 \centering
  \includegraphics[width=1\textwidth]{./Bilder/Procedures_of_IPM.png}\label{Procedures_of_IPM}
  \caption{Procedures of IPM\cite{IPM}}
\end{figure}


%
\section{Edge Detection}\label{sec:Edge Detection}
%

Edge detectors are essential parts of most computer vision systems. Edge detectors dramatically decrease the amount of data to be processed and extract the useful parts of images. They work by detecting discontinuities in brightness. It is usually done by applying a 2-D filter, which is designed in such a way that it is only sensitive to the large gradients present in the image, returning zero values for all monotonous regions. There is a wide variety of specifically composed operators, each of them showing the best results when used with some particular types of edges. For instance, the different orientations of the edges, as well as their geometry, structure, and thickness should correspond with operator characteristics in order for the detection to be reliable. Another problem is the image noise, which in many cases could be confused with the local gradient and thus detected as an edge. Because of that, there is no universally efficient operator, and for each particular case it should be specifically configured, and some pre-editing must be used in order to achieve the best detection quality. In this project, the edge detector is used to detect the lanes and to exclude other unnecessary information from images. There are several different methods suitable for lane edge detection, and all of them can be grouped into two categories. They are :

\begin{enumerate}

\item \textbf{Gradient method : } This method searches for the maximum and minimum in the first derivative of the image and with that, the edges can be found. From the color intensity perspective, any edge has either a rise or fall, which gives a local spike after applying the derivative operator, thus highlighting the edge location in the image. For this method, the first order derivative filter must be used. For example : Sobel-Operator, which is described in detail in Section \ref{sec:Sobel Operator}.
 
\item \textbf{Laplacian method : } This method searches for the zero crossing in the second derivative of the image. This is an alternative for locating local spikes, as the second derivative is equal to zero when the first is at minimum or maximum. For example : Canny edge detector, which is described in detail in Section \ref{sec:Canny Edge Detector}. 
  
\end{enumerate}
 
According to \cite{Machine_Vision}, there are three steps of the edge detection algorithm. They are :

\begin{itemize}

\item \textbf{Filtering : } For edge detection, it is required to use a suitable smoothing filter. The filters sharpen the edges and ignore unnecessary information. It is often utilized to improve the functioning of an edge detector against noise. The more filtering is applied, however, the greater the loss of edge strength. The most common implementation of a smoothing filter used for image preprocessing is Gaussian blur, which is described in detail in Section \ref{sec:Gaussian Blur}   
 
\item \textbf{Enhancement : } To be able to better detect edges, changes in the intensity in the area surrounding a point must be determined. Pixels in which a significant change in intensity occurs are emphasized by enhancement, which is usually applied by calculating the gradient magnitude.
  
\item \textbf{Detection : } Though many points in an image have a nonzero value for the gradient, not all of these points are actually edges. Because only points with strong edge content are desired, a method must be applied to determine which points are actual edge points. Thresholding is often utilized to do so.
 
\end{itemize}

Well known edge detection filters are :

\begin{itemize}

 \item Sobel-Operator
 \item Canny Edge Detector
 \item Laplacian-Filter
 \item Prewitt-Operator
 
 \end{itemize}


%
\subsection{Sobel Operator}\label{sec:Sobel Operator}

The Sobel Operator, sometimes called the Sobel filter or Sobel-Feldmann operator is one of the most used edge detectors in image processing and computer vision. The Sobel Operator uses vertical and horizontal masks. These masks used are odd-numbered square matrices and they are generally 3x3 matrices. Approximations of the derivatives for the horizontal changes and for the vertical changes are calculated by the operator by using two 3x3 kernels and convolving them with the original image. If A is defined as the source, if $G_{x}$ is an image which contains the horizontal derivative approximations at each point, and if $G_{y}$ is an imagine which contains the vertical derivative approximations at each point, then the calculations are\cite{SobelOperatorandCannyEdgeDetector}:

\begin{center}

$  G_{x} = 
  \begin{bmatrix}
	+1 & 0 & -1 \\
	+2 & 0 & -2 \\
	+1 & 0 & -1 \\
   \end{bmatrix} * A  \quad
  G_{y} = 
  \begin{bmatrix}
	+1 & +2 & +1 \\
	0 & 0 & 0 \\
	-1 & -2 & -1 \\
  \end{bmatrix} * A		$

\end{center}

where * here denotes the 2-dimensional signal processing convolution operation.

Since the Sobel kernels can be decomposed as the products of an averaging and a differentiation kernel, they compute the gradient with smoothing. For example, \textbf{$G_{x}$}  can be written as:

\begin{center}
$  \begin{bmatrix}
	+1 & 0 & -1 \\
	+2 & 0 & -2 \\
	+1 & 0 & -1 \\
   \end{bmatrix} =  \begin{bmatrix}
	1 \\
	2 \\
	1 \\
  \end{bmatrix} \begin{bmatrix}
	+1 & 0 & -1
  \end{bmatrix}
$
\end{center}


The x-coordinate is defined here as increasing in the 'right'-direction, and the y-coordinate is defined as increasing in the 'down'-direction. At each point in the image, the resulting gradient approximations can be combined to give the gradient magnitude, using\cite{SobelOperatorandCannyEdgeDetector}:

\begin{center}
G = $\sqrt{ G_{x}^{2} + G_{y}^{2} }$
\end{center}

Using this information, we can also calculate the gradient's direction\cite{SobelOperatorandCannyEdgeDetector}:

\begin{center}
$\theta = atan\bigg(\dfrac{G_{y}}{G_{x}}\bigg)$
\end{center}

where, for example, $\theta$ is 0 for a vertical edge which is lighter on the right side.


\begin{figure}[H]
  \centering
  \subfloat[Original Image]{\includegraphics[width=0.4\textwidth]{./Bilder/Sobel_Original.png}\label{fig:f1}}
  \hfill
  \subfloat[Sobel Operator applied Image]{\includegraphics[width=0.4\textwidth]{./Bilder/Sobel_Operator.png}\label{fig:f2}}
  \caption{Sobel Operator\cite{SobelOperatorandCannyEdgeDetector}}
\end{figure} 


The Sobel operator's main advantages are due to its simplicity. One is that the Sobel operator provides an approximation of the gradient magnitude. Another advantage of the Sobel operator is that it can detect edges and their orientations. In this cross operator, the detection of the edges and their orientations is considered to be simple because of the approximation of the gradient magnitude. The Sobel operator does have some disadvantages, however. For example, it is sensitive to noise. The magnitude of the edges degrades as the level of noise present in image increases. As the magnitude of the edges degrades, the accuracy of the Sobel operator also diminishes. Overall, the Sobel operator cannot accurately detect edges when edges are thin or smooth.



In this master's thesis, the Sobel Operator function in OpenCV was used. Before the Sobel operator was used, in order to reduce the noise, the 'GaussianBlur' function was used. As can perhaps be inferred from the name, the 'GaussianBlur' function blurs the image using a Gaussian filter. In order to apply the 'GaussianBlur' function, the following command must be run.

 \begin{center}
  
\texttt{ void GaussianBlur(cv::Mat src, cv::Mat dst, cv::Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER$\_$DEFAULT )}

  \end{center}

The parameters of the function will be described in detail.\cite{GaussianBlur}
 
\begin{itemize}

\item \textbf{src : }Input image, which can have any number of channels but the depth of which must be one of the following: CV$\_$8U, CV$\_$16U, CV$\_$16S, CV$\_$32F and CV$\_$64F.
 
\item \textbf{dst : }Output image, which must be the same size and type as the input image.

\item \textbf{ksize : }Gaussian kernel size. This size shows the width and height of the Gaussian kernel. These sizes must not be the same value and the values must be odd and positive.

\item \textbf{sigmaX : }Gaussian kernel standard deviation in the X direction.

\item \textbf{sigmaY : }Gaussian kernel standard deviation in the Y direction.

\item \textbf{borderType : }Pixel extrapolation method.

\end{itemize}


Guassian Blur will be explained in the Section \ref{sec:Gaussian Blur}.


After the Gaussian Blur is applied, the picture must be converted from color to grayscale. For that, there is a small function in OpenCV. With the following command, a color picture can be converted easily to grayscale.

 \begin{center}
 
\texttt{ void cvtColor(cv::Mat src, cv::Mat dst, int code, int dstCn=0 )}

 \end{center}
 
 The parameters of the function will be described in detail.
 
 \begin{itemize}

\item \textbf{src : }Input image
 
\item \textbf{dst : }Output image which is the same size and depth as the input image.

\item \textbf{code : }Color space conversion code(here used CV$ \_ $BGR2GRAY).

\item \textbf{dstCn : }Number of channels in the destination image.


\end{itemize}

Before the Sobel operator is used, the Gaussian Filter must be applied and the image must be converted to gray scale. After this, the image is ready for Sobel operator to be applied. In order to calculate the 'derivatives' in the x and y directions, the following command must be run twice because gradient X and gradient Y must be calculated separately.

 \begin{center}

\texttt{ void Sobel(cv::Mat src, cv::Mat dst, int ddepth, int dx, int dy, int ksize=3, double scale=1, double delta=0, int borderType=BORDER$\_$DEFAULT )}

 \end{center}
 
 The parameters of the function will be described in detail.
 
  \begin{itemize}

\item \textbf{src : }Input image.

\item \textbf{dst : }Output image which is in the same size and depth as the input image.

\item \textbf{ddepth : }The depth of the output image.

\item \textbf{xorder : }Order of the derivative x.

\item \textbf{yorder : }Order of the derivative y.

\item \textbf{ksize : }Size of the extended Sobel kernel; it must be 1, 3, 5, or 7.

\item \textbf{scale, delta and borderType : }Optional values. In this project, the default values were used.

  \end{itemize}

The last step of the application of the Sobel operator is approximating the gradient by adding both directional gradients. In the previous step, the gradients of the x and y coordinates were calculated separately. With following command, the weighted sum of these two gradients must be calculated.\cite{addWeighted}

 \begin{center}
 
\texttt{ void addWeighted(cv::Mat src1, double alpha, cv::Mat src2, double beta, double gamma, cv::Mat dst, int dtype=-1)}
 
  \end{center}
  
  The parameters of the function will be described in detail.
  
    \begin{itemize}

\item \textbf{src1 : }First input array.

\item \textbf{alpha : }Weight of the first array elements.

\item \textbf{src2 : }Second input array. This array must have the same size and channel number of the first input array.

\item \textbf{beta : }Weight of the second array elements.

\item \textbf{dst : }Output array, which has the same size and channel number of the the input arrays.

\item \textbf{gamma : }Scalar added to each sum.

\item \textbf{dtype : }Optional depth of the output array.

 \end{itemize}






 
%
\subsection{Canny Edge Detector}\label{sec:Canny Edge Detector}

Another popular edge detector is the Canny Edge Detector, which was developed in 1986 and was named after its developer, John F. Canny. The Canny Edge Detector is a multi-stage algorithm and it can be analyzed in five different stages.\cite{Canny_Edge_Detector2}

\begin{enumerate}
\item \textbf{Noise Reduction : } To get stable lane detection results, we have to reduce/remove all noise from frames. Lane detection without filtering out the noise can cause false detection. Gaussian filter is used for removing noise in the frames. Gaussian filter blurs images and removes detail and noise. The size of Gaussian filter kernel must be (2k+1)x(2k+1). It is important to choose the size of Gaussien filter because if the size of kernel is larger, detector's sensitivity to noise is lower but on the other hand, with the increase in size of the Gaussian filter kernel, the localization error in the edge detection will also increase slightly. \cite{Canny_Edge_Detector}



\item \textbf{Finding Intensity Gradient of the Image : } Essentially, the Canny algorithm locates edges in image where the grayscale intensity changes most starkly. In order to find these areas, the gradients of the image must be determined. In order to determine the gradients at each pixel in the smoothed image, the Sobel operator is applied. The Sobel operator has already been thoroughly discussed in section \ref{sec:Sobel Operator}.

\item \textbf{Non-maximum Suppression : } Non-maximum suppression is an edge thinning technique which is used as an intermediate step in many computer vision algorithms. The image is scanned along the image gradient direction, and pixels that are not part of the local maxima are set to zero. This way, all image information that is not part of the local maxima is effectively suppressed.

\item \textbf{Double Thresholding : } The edge pixels remaining after applying non-maximum suppression provide a more accurate depiction of real edges in an image. Despite this, there are still some remaining edge pixels resulting from noise and color variation. Therefore, it is necessary to filter out edge pixels with a weak gradient value while preserving edge pixels with a high gradient value. In order to do this, high and low threshold values must be selected. Edge pixels are marked as strong edge pixels when gradient values are higher than the high threshold value. They are marked as weak edge pixels when gradient values are lower than the high threshold value and higher than the low threshold value. They are suppressed when their values are lower than the low threshold value. The two threshold values are determined empirically and are dependent on the content of a given image.


\item \textbf{Hysteresis Thresholding : } Hysteresis Thresholding is the last part of Canny Edge Detector. Until this step, strong edge pixels are extracted from the true edges but there are also some weak edge pixels, some of them are extracted from true edges and some of them are extracted from some noise. So the weak edge pixels which are extracted from true edge, should be strong edge pixels and the weak edge pixels which are extracted from noises must be removed. If there is a weak edge pixel, eight neighbor pixels of that weak edge pixel are checked, and if at least one of these neighbor pixels is a strong edge pixel, these weak edge pixels remain as edges in the final image. 

\end{enumerate}


\begin{figure}[H]
  \centering
  \subfloat[Original Image]{\includegraphics[width=0.4\textwidth]{./Bilder/Sobel_Original.png}\label{fig:f1}}
  \hfill
  \subfloat[Canny Edge Detector applied Image]{\includegraphics[width=0.4\textwidth]{./Bilder/Canny_Edge_Detector.png}\label{fig:f2}}
  \caption{Canny Edge Detector\cite{Canny_Edge_Detector}}
\end{figure} 



The main advantage of Canny Edge Detector is enhancement of the signal with respect to the noise ratio. This is done via the non-maxima suppression method, as it results in one pixel wide ridges as the output. The other advantage is better detection of edges, especially in a noisy state, by applying the thresholding method. The effectiveness of the Canny method is affected by adjustable parameters. Small filters are desirable for the detection of small, sharp lines, since they cause fewer instances of blurring. Large filters are desirable for detecting larger, smoother edges. However, they also cause higher instances of blurring. 
The main disadvantage of the Canny edge detector is that it is time consuming, due to its complex computation.



The Canny Edge Detector is also a function in OpenCV. As is the case with the Sobel Operator, before the Canny Edge Detector is used, a filter should be applied (for example, the Gaussian Filter) and the image must be converted to gray scale. With the following command, the Canny Edge Detector can be run in OpenCV.\cite{Canny_Edge_Detector3}

 \begin{center}
 
\texttt{ void Canny(cv::Mat image, cv::Mat edges, double threshold1, double threshold2, int apertureSize=3, bool L2gradient=false)}

 \end{center}
 
 The parameters of the function will be described in detail.\cite{Canny_Edge_Detector3}
 
     \begin{itemize}

\item \textbf{image : }Input image, which has to have an 8-bit single channel.

\item \textbf{edges : }Output image, which is the same size and type.

\item \textbf{threshold1 : }First threshold of the hysteresis procedure.

\item \textbf{threshold2 : }Second threshold of the hysteresis procedure.

\item \textbf{apertureSize : }Size of the extended Sobel kernel.

\item \textbf{L2gradient : }A flag for image gradient magnitude.

\end{itemize}
 
%



In this master's thesis the Sobel Operator was utilized. The first reason for using this algorithm rather than the Canny Edge Detection algorithm is its performance. The latter uses the second derivative, which is more computationally expensive and therefore takes more time. For the purpose of lane detection, it is crucial to spend as little computation time per frame as possible, because longer durations may result in a substantial delay between the image capture and the car driving module's reaction to a new lane position. 




%
\section{Gaussian Blur}\label{sec:Gaussian Blur}

The Gaussian Blur is a non-uniform low pass filter which is used to blur images. For smoothing/blurring the images, there are also some other filters, like Normalized Box Filter, Median Filter, Bilateral Filter, as well as others, but the Gaussian Blur is the most useful one, although it is not the fastest. This filter is widely used in image processing as one part of the preprocessing stage because it reduces image noise and detail.

The Gaussian Blur uses a Gaussian function in order to calculate the transformation to apply to each pixel in the image. The equation of a Gaussian function in one dimension has the following form\cite{GaussianBlur}: 

 \begin{center}
 \scalebox{1.2}{$ G(x) = \dfrac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2\sigma^{2}}} $} 
\end{center}

where $ \sigma $ is the standard deviation of the distribution. The equation of a Gaussian function in two dimensions is the product of two such Gaussians, one in each dimension\cite{GaussianBlur}: 

 \begin{center}
\scalebox{1.25}{$ G(x,y) = \frac{1}{2\pi\sigma^{2}}e^{-\frac{x^{2}+y^{2}}{2\sigma^{2}}} $}
\end{center}



\begin{figure}[H]
  \centering
  \subfloat[1-D Gaussian distribution]{\includegraphics[width=0.5\textwidth]{./Bilder/GaussianBlur_1.png}\label{fig:f1}}
  \hfill
  \subfloat[2-D Gaussian distribution]{\includegraphics[width=0.5\textwidth]{./Bilder/GaussianBlur_2.png}\label{fig:GaussianBlur}}
  \caption{Gaussian distributions with mean (0,0) and $\sigma$=1\cite{GaussianBlur}}
\end{figure} 



The Gaussian filter works by using the 2D distribution as a point-spread function. This is achieved by convolving the 2D Gaussian distribution function with the image. A discrete approximation to the Gaussian function needs to be produced. This theoretically requires an infinitely large convolution kernel, as the Gaussian distribution is non-zero everywhere. Fortunatly, the distribution has approached very close to zero at about three standard deviations from the mean. 99\% of the distribution falls within three standard deviations. This means that normally, the kernel size can be limited to contain only values within three standard devations of the mean.

The distribution is assumed to have a mean of zero. The continuous Gaussian functions need to be discretized to store it as discrete pixels. An integer valued 5 by 5 convolution kernel approximating a Gaussian with a $ \sigma $ of 1 is shown\cite{GaussianBlur} : 




\begin{center}
\scalebox{2}{$\frac{1}{273}$}
\begin{tabular}{ | c | c | c | c | c |} \hline
  
  1 & 4  & 7  & 4  & 1  \\ \hline  
  4 & 16 & 26 & 16 & 4   \\ \hline  
  7 & 26 & 41 & 26 & 7   \\ \hline 
  4 & 16 & 26 & 16 & 4   \\ \hline 
  1 & 4  & 7  & 4  & 1  \\ \hline   
    
      \end{tabular}
  \label{tab:GaussianBlur}
\end{center}


As seen in the convolution kernel shown above, the kernel coefficients diminish with increasing distance from the kernel's center and the central pixels have a higher weighting than those on the periphery. Also, the Gaussian kernel coefficients depend on the value of $\sigma$. If the value of $\sigma$ is larger, it produces a wider peak, meaning that the images are blurred more.





















\section{Hough-Transformation}\label{sec:Hough-Transformation}
%
In order to isolate features of a particular shape in an image, a method called the Hough Transformation can be utilized. The Hough Transformation which is universally used today was further developed by Richard O. Duda and Peter E. Hart in 1972, although a more rudimentary version had been patented by Paul Hough in 1962.\cite{Hough_Transformation} In computer vision, it is often necessary to detect simple edges like straight lines, curves, and ellipses. As a result, this technique is used often in computer vision and image processing. The Hough Transformation is used mostly after an edge detection algorithm. There are actually variations of the Hough Transformation. In this master's thesis, two variations of the Hough Transformation were used. They are:

\begin{itemize}

\item \textbf{Standard Hough Line Transformation : }This type of Hough Transformation is used mostly for detecting straight lines. How the Standard Hough Transformation works will be explained in detail and as a result, why it is more suitable for detecting straight lines will become more clear.

\item \textbf{Probabilistic Hough Transformation : }This type of Hough Transformation is suitable for both straight lines and curves, so in this master's thesis, the Probabilistic Hough Transformation was used for detecting lanes. How the Probabilistic Hough Transformation works will also be explained.

\end{itemize}
%
\subsection{Standard Hough Line Transformation}\label{sec:Standard Hough Line Transformation}
%
The Standard Hough Line Transformation, which is also called the Linear Hough Transformation, is used mostly for detecting straight lines. There are many different formulas to represent a line segment analytically. A convenient formula for representing a line in an Image(Cartesian) space is:

  \begin{center}

\scalebox{1.1}{$ \rho = x cos \theta + y sin \theta $}

  \end{center}
  
In this equation, \textit{$ \rho $} is the length of a normal from the origin to the line and \textit{$ \theta $} is the angle between this normal and the x-axis.
 

 
Standard Hough Transformation algorithm consists of the following steps\cite{Standard_Hough}:
 
\begin{algorithm}[h]

 \caption{ HT for detecting lines on the $\theta - \rho$ parameterization}
\begin{algorithmic}[1]
        \Procedure{Detect lines}{}\newline
        \textbf{Input: }Input image I with dimensions $I_{w},I_{h}$, Hough space dimensions $H_{\rho},H_{\theta}$.\newline
        \textbf{Output:} Detected lines $L=\lbrace(\theta_{1},\rho_{1}),...\rbrace$ 
        \State{$H(\overline{\rho},\overline{\theta})\longleftarrow$ 0, $\forall \overline{\rho} \in \lbrace$1,...,$H_{\rho}\rbrace$,$\overline{\theta}\in\lbrace$1,...,$H_{\theta}\rbrace$}
        \For {\textbf{all} $x\in \lbrace$1,...,$I_{w}\rbrace$,$y\in\lbrace$1,...,$I_{h}\rbrace$}
         \If {$I(x,y)$ \textbf{is edge}}
         \State{\textbf{increment} $H(\overline{\rho}(\overline{\theta},x,y),\overline{\theta}),\forall\overline{\theta}\in\lbrace1,...H_{\theta}\rbrace$}
         \EndIf
         \EndFor
	      \State{$L=\lbrace(\theta(\overline{\theta}),\rho(\overline{\rho}))\vert\overline{\rho}\in\lbrace1,...,H_{\rho}\rbrace\wedge\overline{\theta}\in\lbrace1,...,H_{\theta}\rbrace\wedge$ at $(\overline{\rho},\overline{\theta})$  is a high max. in $H\rbrace$ }
        
        \EndProcedure
 \end{algorithmic}    
 \end{algorithm}
 
 
An edge detection algorithm is applied to the image with dimensions $I_{w}, I_{h}$, which provides the binary data used in Step 4 to determine whether or not a pixel belongs to an edge. Steps 3 - 7 convert detected edges to the Hough space $H$ and accumulate them. The following function is used to calculate $\overline{\rho}$ for all detected edges that pass through the point $(x,y)$ at angle $\overline{\theta}$\cite{Standard_Hough}: 
 
 
 \begin{center}
 
 
$ \overline{\rho}(\overline{\theta},x,y) = \Bigg[\dfrac{H_{\rho}\big((y-\dfrac{I_{h}}{2})sin(\dfrac{\pi}{H_{\theta}}\overline{\theta})+(x-\dfrac{I_{w}}{2})cos(\dfrac{\pi}{H_{\theta}}\overline{\theta})\big)}{\sqrt{I^{2}_{w}+I^{2}_{h}}}\Bigg] $ 
 \end{center}
 
In Step 8, the high maximum (the local maximum exceeding a given threshold) of all accumulated lines in Hough space $H$ is determined. It gives $\overline{\rho}$ and $\overline{\theta}$ as detected line parameters in the Hough space. After that Hough coordinates $\overline{\rho}$ and $\overline{\theta}$ are transformed back into $\rho$ and $\theta$ using the following formulas\cite{Standard_Hough}:   
  
 \begin{center}

$ \rho(\overline{\rho}) = \dfrac{\sqrt{I^{2}_{w} + I^{2}_{h} }}{H_{\rho}} \bigg(\overline{\rho}-\dfrac{H_{\rho}}{2}\bigg)   ,  \qquad              \theta(\overline{\theta})=\dfrac{\pi}{H_{\theta}}\overline{\theta} $
 
 \end{center}
 
 \begin{figure}[H]
 \centering
  \includegraphics[width=0.9\textwidth]{./Bilder/Standard_Hough_Line_Transformation.png}
  \caption{Standard Hough Line Transformation\cite{Standard_Hough_Transformation}}
  \label{Standard_Hough_Line_Transformation_fig}
\end{figure}
 
 In the left side of Figure \ref{Standard_Hough_Line_Transformation_fig}, a line is shown in an Image (Cartesian) space and on the right side of same Figure, the same line is shown in its transformed state in the Hough space. A point in the Image (Cartesian) space is transformed into a sinusoidal curve in the Hough space. A line can be defined as a set of points, so a line is transformed into a set of sinusoids intersecting at a point in the Hough space. In this case, detecting points in the Hough space also means detecting the lines in the Cartesian space. After detecting points in the Hough Space, the points must be inversely transformed in order to get the corresponding lines in the Cartesian space.
 
Standard Hough Line Transformation is widely used technique in curve detection. Despite this, it does have two important downsides. Firstly, for each nonzero pixel in an image, the parameters for both the existing curve as well as the redundant ones are obtained during the 'voting procedure'. Secondly, the more accuracy that is needed, the higher the parameter resolution must be defined. As a result, there is often a large storage requirement and low speed for real applications. Probabilistic Hough Transformation was derived in order to counter these problems. 

In this master's thesis, in order to detect the Hough lines, the function of \textit{cv2.HoughLines()} in OpenCV was used. The function is:

  \begin{center}
  
\texttt{ void HoughLines(cv::Mat image, cv::Mat lines, double $\rho$, double $\theta$, int threshold, double srn=0, double stn=0)}

  \end{center}

The parameters of the function will be described in detail.\cite{Standard_Hough_Transformation2}
 
\begin{itemize}

\item \textbf{image : }Output of the edge detector. It should be a grayscale image (although in fact it is a binary one).
 
\item \textbf{lines : }A vector that will store the parameters ($ \rho $, $ \theta $) of the detected lines. At the end of the Standard Hough Line Transformation, only the $ \rho $ and $ \theta $ values will be returned, so only straight lines can be drawn in OpenCV.

\item \textbf{$ \rho $ : }The resolution of the parameter $ \rho $ in pixels. In this master's thesis, \textbf{1 pixel} was used.

\item \textbf{$ \theta $ : }The resolution of the parameter $ \theta $ in radians. In this master's thesis, \textbf{180 degree}(CV$ \_ $PI) was used.

\item \textbf{threshold : }The minimum number of intersections to 'detect' a line.

\item \textbf{srn and stn : }Both of these parameters are for the multi-scale Hough transformation. srn is a divisor for the distance resolution $ \rho $ and stn is a divisor for the distance resolution $ \theta $. Default values of both parameters are zero.

\end{itemize}

%
\subsection{Probabilistic Hough-Transformation}\label{sec:Probabilistic Hough-Transformation}
%

Probabilistic Hough Transformation, also called Randomized Hough Transformation, is a variant of the Standard Hough Transformation. Hough Transformation is computationally expensive. Probabilistic Hough Transformation is an optimization of Standard Hough Transformation which does not analyze all points in the image. Instead, it looks at only a random subset of points sufficient for line detection. In order to apply Probabilistic Hough Transformation, the threshold must be decreased. Probabilistic Hough Transformation is commonly used to detect curves (straight line, circle, elipse,etc.) 




Essentially, the point of the Hough Transformation is to implement a 'voting procedure' for all potential curves in the image. Curves that actually found in the image will then have relatively high 'voting scores' when the algorithm terminates. Probabilistic Hough Transformation differs from Standard Hough Line Transformation by trying to avoid a computationally expensive 'voting process' for every nonzero pixel in the image. It does this by taking advantage of the geometric properties of analytical curves, which allows it to improve efficiency as well as reduce the storage requirement of the original algorithm.


 \begin{figure}[H]
 \centering
  \includegraphics[width=0.9\textwidth]{./Bilder/Probabilistic_HT.png}
  \caption{Probabilistic Hough Transformation}
  \label{Probabilistic_HT_fig}
\end{figure}

Line detection using Probabilistic Hough Transformation is similar to standard Hough Transformation, but instead of using all edge points of the image, some subset is randomly selected, which is then used to determine a single unique parameter space point for every pair of points from the subset (see Figure \ref{Probabilistic_HT_fig}).

As soon as the amount of points that are mapped to the same parameter in Hough space exceeds a certain threshold, the line is confirmed to be present in the original image, i.e. the line is detected. It is then removed from the input array so that the other lines are detected faster. The algorithm terminates either after all the lines are detected and there is no more input data, or the maximum limit of iterations is reached. Because of this approach, the required computational power and memory consumption are much less when compared with Standard Hough Transformation.


Probabilistic Hough Transformation algorithm consists of the following steps:

\begin{enumerate}

\item Initialise the set of points K representing the detected edges of the input image
\item  Randomly select two points from the set K  
\item  Apply the mapping into Hough space and determine the parameter point of the straight line represented by two selected points
\item  Acumulate parameter points in the Hough space. If  the amount of input points mapping to the same Hough space parameter exceed a given threshold, consider this parameter as a parameter for detected line and remove it from the input data, otherwise go to step 2, if the maximum number of loops is not reached.
\item  Repeat until all input data is processed.
\end{enumerate}

The following function from OpenCV library implements aforementioned algorithm:

  \begin{center}
  
\texttt{void HoughLinesP(cv::Mat image, cv::Mat lines, double $\rho$, double $\theta$, int threshold,  double minLinLength=0,  double maxLineGap=0)  }

  \end{center}

The parameters of the function will be described in detail.\cite{Standard_Hough_Transformation2}
 
\begin{itemize}

\item \textbf{image : }Output of the edge detector. It should be a grayscale image (although in fact it is a binary one).
 
\item \textbf{lines : }A vector that will store the parameters ($ x_{start} $, $ y_{start} $, $ x_{end} $, $ y_{end} $) of the detected lines.

At the end of the Probabilistic Hough Transformation, the end points of detected lines will be returned. With the Probabilistic Hough Transformation in OpenCV, straight lines can be detected. In addition, the small straight lines in other shapes can be combined, and with this technique, all shapes can be detected.  

In this master's thesis, all lanes were detected with Probabilistic Hough Transformation. However, instead of Hough Lines, the points which were returned as the result of Probabilistic Hough Transformation were used.

\item \textbf{$ \rho $ : }The resolution of the parameter $ \rho $ in pixels. In this master's thesis, \textbf{1 pixel} was used.

\item \textbf{$ \theta $ : }The resolution of the parameter $ \theta $ in radians. In this master's thesis, \textbf{1 degree}(CV$ \_ $PI/180) was used.

\item \textbf{threshold : }The minimum number of intersections to 'detect' a line.

\item \textbf{minLinLength : }The minimum number of points that can form a line. Lines with less than this number of points are disregarded.

\item \textbf{maxLineGap : }The maximum gap between two points to be considered in the same line.

\end{itemize}



%
\section{K-Nearest Neighbors Algorithm}\label{sec:K-Nearest Neighbors Algorithm}
%
K-Nearest Neighbor(KNN) is an non-parametric lazy learning algorithm. The non-parametric technique means that it doesn't make any assumptions on the underlaying data distribution. In the definition of KNN, the term 'lazy learning algorithm' is used. It means it doesn't use the data training points to do any generalization. In other words, there is no explicit training phase or it is very minimal.  It also means that the training phase is pretty fast. Most of the lazy algorithms -- especially KNN -- make decisions based on the entire training data set. On the other hand, KNN is one of the top 10 data mining algorithms\cite{k_nearest_neighbors}.

\begin{figure}[H]
 \centering
  \includegraphics[width=0.8\textwidth]{./Bilder/k-nearest-neighbors.png}\label{Procedures_of_IPM}
  \caption{K-Nearest-Neighbors Algorithm\cite{k_nearest_neighbors_wikipedia}}
\end{figure}

KNN makes predictions through the direct use of the training dataset. Predictions are made for a new instance (x) by searching the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this could be the mean output variable. In classification, this could be the most common (mode) class value. In order to determine which of the K instances in the training dataset are the closest to a new input, various distance measure methods are used. There are many different distance measure methods, but some important ones are:

\begin{itemize}

\item \textbf{Euclidean Distance : }It is one of the most popular methods for distance measure. In this method, the square root of the sum of the squared differences between a new point and an existing point is calculated.

  \begin{center}
  
\textbf{Euclidean} \scalebox{1.25}{$ \sqrt{\sum_{i=1}^k  (x_{i} - y_{i})^2 } $}

  \end{center}
  


\item \textbf{Manhattan Distance : }It is called also City Block Distance. It calculates the distance between real vectors using the sum of their absolute difference.

  \begin{center}
  
\textbf{Manhattan} \scalebox{1.25}{$\sum_{i=1}^k \begin{vmatrix}x_{i} - y_{i} \end{vmatrix}$}

  \end{center}
  

\item \textbf{Minkowski Distance : }The Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance.
  
  \begin{center}
  
\textbf{Minkowski} \scalebox{1.25}{$ \bigg(\sum_{i=1}^k \Big(\begin{vmatrix}x_{i} - y_{i} \end{vmatrix}\Big)^q\bigg)^\frac{1}{q}$}

  \end{center}

\end{itemize}

There are also some other distance measure methods like Tanimoto, Jaccard, Mahalanobis, and cosine distance. It must be decided which distance measure method should be used according the properties of the dataset. For example, if the input variables are similar in type (e.g. all measured widths and heights), the Euclidean distance measure method is good. However, if the input variables are not similar in type (such as age, gender, height, etc.), the Manhattan distance measure method is better than the Euclidean measure distance method.



The K-Nearest Neighbors Algorithm has advantages and disadvantages. According to \cite{k_nearest_neighbors_adv_disadv},the main advantages of KNN are simplicity, effectiveness, intuitiveness and competitive classification performance in many domains. On the other hand, KNN can have poor run-time performance when the training set is large. It is very sensitive to irrelevant or redundant features because all features contribute to the similarity and thus to the classification. The computation cost is also quite high because we need to compute distance of each query instance to all training samples. 

K-Nearest Neighbors Algorithm is also a function in OpenCV. The following command shows the usage of the KNN function in OpenCV.

\begin{center}

\texttt{void flann::Index$\_$<T>::knnSearch(vector<T>$\&$ query, vector<int>$\&$ indices, vector<float>$\&$ dists, int knn, SearchParams$\&$ params)}

\end{center}

The parameters of the function will be described in detail.\cite{k_nearest_neighbors_OpenCV}

\begin{itemize}

\item \textbf{query : }The query point.

\item \textbf{indices : }The indices of the KNN are found in this vector.

\item \textbf{dists : }The distances of the KNN are found in this vector.

\item \textbf{knn : }Number of nearest neighbors to search for.

\item \textbf{params : }There are some different optional parameters, which can be used in this function.

\end{itemize}


%
\section{Curve Fitting}\label{sec:Curve Fitting}

Curve fitting is used to find the 'best fit' line or curve for a series of data points. Curve fitting results in the production of mostly mathematical equations which can be used to locate points anywhere along the curve. They are several different types of curve fitting. Some of them are\cite{Curve_Fitting}: linear, exponential, polynomial, exponential, power, logarithmic, etc. In this master thesis, polynomial curve fitting was used. Polynomial curve fitting differs from order of the polynomial. Polynomial curve fittings are called different names depending on their orders. First order polynomial curve fittings are called linear regression; second order, quadratic regression; and third order, cubic regression.


\begin{figure}[H]
 \centering
  \includegraphics[width=0.9\textwidth]{./Bilder/Curve_Fitting_Polynomial.png}\label{Curve_Fitting_Polynomial}
  \caption{Types of Polynomial Curve Fitting\cite{Curve_Fitting_Polynomial}}
\end{figure}


In this master's thesis, curve fitting is used for getting the best mathematical descriptions of lanes. Curve fitting uses the Hough points which appear on the lanes as input, and give a mathematical equation as output. First order polynomial curve fitting is more suitable for defining straight lines and second order polynomial curve fitting is more suitable for defining curves. There are both straight and curved lanes, so first order polynomial curve fitting is not sufficient for our project. Accordingly, in this master's thesis, second order polynomial curve fitting is used.

There are many different methods for curve fitting. One of the most famous methods is the least squares method, which is also the method utilized in this master's thesis. Another method which can be used instead of least squares algorithm is the interpolation algorithm. As in the curve fitting method, there are different types of interpolation. Some of them are: linear interpolation, polynomial interpolation, and cubic-spline interpolation. In the interpolation method, the data points are connected to each other. If it is linear interpolation, the data points are connected to each other with a straight line, but if it is cubic-spline interpolation, the data points are connected to each other with a third-degree polynomial. Curve fitting and interpolation each have their own advantages and disadvantages. For example, in curve fitting, the fitted curve does not always pass through the original data points, but in interpolation, the generated curve always does so. On the other hand, if the data set is very large, curve fitting can give the 'best fit' even in low-order polynomials. In this master's thesis, because there are so many data points from the lanes, the curve fitting method is more suitable when compared with interpolation. In addition, if there is noise that occurs far away from the lanes, because of the large size of the data set, this noise is ignored and does not change the fitted curve. In Figure \ref{Interpolation and Curve Fitting to Random Numbers}, there is a fitted curve and interpolation for a small data set. 



\begin{figure}[H]
 \centering
  \includegraphics[width=0.8\textwidth]{./Bilder/CurveFittingvsInterpolation.png}
  \caption{Interpolation and Curve Fitting to Random Numbers\cite{Interpolation_and_CurveFitting}}
  \label{Interpolation and Curve Fitting to Random Numbers}
\end{figure}






Next, how a polynomial curve fitting is generated using the least squares method will be described.

A data set mainly expresses the relationship between variables with an equation which is generally represented with a $k^{th}$ order polynomial. The general description of $ k^{th} $ is :

\begin{center}
\scalebox{1.2}{y = $ a_{k}x^{k} + ... + a_{1}x + a_{0} + \epsilon $}
\end{center}

The general polynomial regression model with the error $\epsilon$ typically provides an estimate rather than an implicit value of the dataset for any given value of x. A data set which has N data points can be described with the maximum order of the polynomial which is k = N - 1, but the lowest possible order of the polynomial is generally chosen.

The aim of the least squares' method is to minimize the variance between dataset values and estimated values from the polynomial equation.

In order to determine the coefficients of the polynomial regression model ($ a_{k}, a_{k-1}, ..., a_{1} $) must be solved the following linear equations.

\begin{center}
 $
 \begin{bmatrix}
N & $$\sum_{i=1}^{N} x_{i}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k}$$ \\
$$\sum_{i=1}^{N} x_{i}$$ & $$\sum_{i=1}^{N} x_{i}^2$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
$$\sum_{i=1}^{N} x_{i}^{k}$$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{2k}$$ \\

\end{bmatrix}  \begin{bmatrix}
	 a_{0}  \\
	 a_{1}  \\
	 \vdots  \\
	 a_{k}  \\
  \end{bmatrix} = 
  \begin{bmatrix}
	 $$\sum_{i=1}^{N} y_{i}$$  \\
	 $$\sum_{i=1}^{N} x_{i}y_{i}$$  \\
	 \vdots  \\
	 $$\sum_{i=1}^{N} x_{i}^{k}y_{i}$$  \\
  \end{bmatrix}
$ 
\end{center}

The equations in the stardard form of Ma = b can be solved with many different methods. In this master's thesis, \textit{Cramer's Rule} was used to solve for the polynomial coefficients of curve fitting. With the help of the determinants of the square matrix, M can be solved for in any linear system of equations in order to find the coefficients. Each of the coefficients $a_{k}$ may be determined using the following equation\cite{Curve_Fitting2}:

\begin{center}
\scalebox{1.2}{$a_{k} = \dfrac{det(M_{i})}{det(M)}$}
\end{center}

In order to find the coefficients of polynomial curve fitting, we have to use the equation shown above, so the determinate of $ M_{i} $ must be divided by the determinate of M. Above, the equation Ma = b was shown, so the determinate of the M matrix can be calculated. However, for the determinate of $ M_{i} $ matrix, the M matrix must be modified. To find the $ M_{i} $ matrix, the $ i^{th} $ column must be replaced with the column vector b, which was used in the equation Ma = b. For example, if the $ M_{0} $ matrix is to be determined, the M matrix must be modified like at the following\cite{Curve_Fitting2} :


\[
 M = \begin{bmatrix}
N & $$\sum_{i=1}^{N} x_{i}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k}$$ \\
$$\sum_{i=1}^{N} x_{i}$$ & $$\sum_{i=1}^{N} x_{i}^2$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
$$\sum_{i=1}^{N} x_{i}^{k}$$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{2k}$$ \\

\end{bmatrix} \quad
 M_{0} = \begin{bmatrix}
$$\sum_{i=1}^{N} y_{i}$$ & $$\sum_{i=1}^{N} x_{i}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k}$$ \\
$$\sum_{i=1}^{N} x_{i}y_{i}$$ & $$\sum_{i=1}^{N} x_{i}^2$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
$$\sum_{i=1}^{N} x_{i}^{k}y_{i}$$ & $$\sum_{i=1}^{N} x_{i}^{k+1}$$ & $\dots$ & $$\sum_{i=1}^{N} x_{i}^{2k}$$ \\

\end{bmatrix}
\]

In this master's thesis, the $ 2^{nd} $ order polynomial equation is used, so only the values of $ a_{0}, a_{1} and a_{2} $ must be calculated. To understand curve fitting better, we are going to develop the $ 2^{nd} $ order polynomial curve fit for the given dataset.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | c | c |}
    \hline
    x & -3  &  -2  &  -1  & -0.2  &  1  &  3 \\ \hline
    y & 0.9 &  0.8 &  0.4 &  0.2  & 0.1 &  0 \\
    \hline
  \end{tabular}
\end{center}

\[
 M = \begin{bmatrix}
6 & -2.2 & 24.04 \\
-2.2 & 24.04 & -8.008 \\
24.04 & -8.008 & 180.0016 \\

\end{bmatrix} \quad
 M_{0} = \begin{bmatrix}
2.4 & -2.2 & 24.04 \\
-4.64 & 24.04 & -8.008 \\
11.808 & -8.008 & 180.0016 \\

\end{bmatrix} 
\]

\[
 M_{1} = \begin{bmatrix}
6 & 2.4 & 24.04 \\
-2.2 & -4.64 & -8.008 \\
24.04 & 11.808 & 180.0016 \\

\end{bmatrix} \quad
 M_{2} = \begin{bmatrix}
6 & -2.2 & 2.4 \\
-2.2 & 24.04 & -4.64 \\
24.04 & -8.008 & 11.808 \\

\end{bmatrix}
\]

\begin{center}

$ a_{0} = \dfrac{det(M_{0})}{det(M)}   \Longrightarrow  a_{0} = \dfrac{ 2671.1962}{11661.2736} = 0.2291$  \vfill 
$ a_{1} = \dfrac{det(M_{1})}{det(M)} \Longrightarrow  a_{1} = \dfrac{ -1898.4602}{11661.2736} = -0.1628$  \vfill 
$ a_{2} = \dfrac{det(M_{2})}{det(M)} \Longrightarrow  a_{2} = \dfrac{ 323.7632}{11661.2736} = 0.0278$


\end{center}

In this case, the fitted curve function is : \vfill 
\begin{center}
y = 0.0278$x^{2} $ - 0.1628x +0.2291
\end{center}

\begin{figure}[H]
 \centering
  \includegraphics[width=0.8\textwidth]{./Bilder/Curve_Fitting.png}\label{Curve_Fitting}
  \caption{Curve Fitting\cite{Curve_Fitting_Plot}}
\end{figure}




%




